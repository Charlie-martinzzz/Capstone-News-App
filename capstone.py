# -*- coding: utf-8 -*-
"""Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11iN68ngRgAXF85cB3qxDE7hf9VZmYQ42

# **Python script for taking data from google news API and uploading to postgresql database**
"""

# Firstly import all the neccesary libraries

import requests
import pandas as pd
import os
from dotenv import load_dotenv
load_dotenv()
import psycopg2 as psql
from psycopg2.extras import execute_values

# Import neccesary sensitive information secretly with os and dotenv

password = os.getenv('sql_password')
user = os.getenv('sql_user')
my_host = os.getenv('host')
API_key = os.getenv('api_key')

# Create a get request for the API

news_url = f'https://serpapi.com/search?engine=google_news&gl=uk&api_key={API_key}'

response = requests.get(news_url).json()

# Here we will extract the desired data from the API

# First, initiate the empty list articles to store data

articles = []

# Loop through everything in news_results
for article in response['news_results']:
    # Check if the article has necessary fields
    if 'title' in article and 'source' in article and 'link' in article and 'date' in article:
        title = article['title']
        source = article['source']['name']
        icon = article['source']['icon']
        link = article['link']
        date = article['date']
        articles.append((title, source, icon, link, date))

    # 'Stories' are similar, unique articles that are nested within a news_result story, and need to be accessed separately
    if 'stories' in article:
        for story in article['stories']:
            if 'title' in story and 'source' in story and 'link' in story and 'date' in story:
                title = story['title']
                source = story['source']['name']
                icon = story['source']['icon']
                link = story['link']
                date = story['date']
                articles.append((title, source, icon, link, date))

# Convert the list to a pandas DataFrame
columns = ['title', 'source', 'icon', 'link', 'date']
news_df = pd.DataFrame(articles, columns=columns)

# the date column needs to be cleaned to only contain the date

# Split the string at the first comma and keep only the date part
news_df['date'] = news_df['date'].str.split(',', n=1).str[0]

# Convert to datetime and extract the date part
news_df['date'] = pd.to_datetime(news_df['date'], format='%m/%d/%Y').dt.date

"""# SQL
Now the data is prepped and ready to be sent to the SQL database
"""

# First, a connection to the SQL databse is established with the neccesary credentials

conn = psql.connect(database = "pagila",
                    user = user,
                    password = password,
                    host = my_host,
                    port = 5432
                    )

# Create cursor
cur = conn.cursor()

# Prepare insert query with conflict handling
# the title column in the SQL database has a condition that each entry has -
# to be unique, and on conflict with this the tuple of data wont be added

insert_query = """
INSERT INTO student.capstone_charlie (title, source, icon, link, date)
VALUES %s
ON CONFLICT (title) DO NOTHING;
"""

# Convert dataframe to list of tuples
data_tuples = list(news_df.itertuples(index=False, name=None))

# Execute the insert query
execute_values(cur, insert_query, data_tuples)

# Commit the transaction
conn.commit()

# Close cursor and connection
cur.close()
conn.close()
